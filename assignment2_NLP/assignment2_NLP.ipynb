{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4c2c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk scikit-learn gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d8f5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ecc4312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 200\n"
     ]
    }
   ],
   "source": [
    "data = fetch_20newsgroups(subset='train')\n",
    "\n",
    "documents = data.data[:200]   # taking first 200 documents for faster execution\n",
    "\n",
    "print(\"Number of documents:\", len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93b7fd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Document:\n",
      "\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Document:\\n\")\n",
    "print(documents[0][:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0ca1dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 1000\n",
      "BoW Matrix Shape: (200, 1000)\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
    "bow_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Vocabulary Size:\", len(count_vectorizer.get_feature_names_out()))\n",
    "print(\"BoW Matrix Shape:\", bow_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cd4e880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Matrix (first 5 documents):\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Bag of Words Matrix (first 5 documents):\")\n",
    "print(bow_matrix[:5].toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6277a673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized BoW Matrix (first 5 documents):\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "bow_norm_matrix = normalize(bow_matrix, norm='l1')\n",
    "\n",
    "print(\"Normalized BoW Matrix (first 5 documents):\")\n",
    "print(bow_norm_matrix[:5].toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83c1d046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (200, 1000)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"TF-IDF Matrix Shape:\", tfidf_matrix.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3102195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix (first 5 documents):\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF Matrix (first 5 documents):\")\n",
    "print(tfidf_matrix[:5].toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "830427f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sample:\n",
      "['from', ':', 'lerxst', '@', 'wam.umd.edu', '(', 'where', \"'s\", 'my', 'thing', ')', 'subject', ':', 'what', 'car', 'is', 'this', '!', '?', 'nntp-posting-host', ':', 'rac3.wam.umd.edu', 'organization', ':', 'university', 'of', 'maryland', ',', 'college', 'park']\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs = [\n",
    "    nltk.word_tokenize(doc.lower())\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "print(\"Tokenized Sample:\")\n",
    "print(tokenized_docs[0][:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06ec00de",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_docs,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8ca0bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for word 'computer':\n",
      "[-8.4389988e-03  2.0750697e-01 -1.6187280e-01  5.7690009e-02\n",
      "  1.3881016e-01 -6.4306951e-01  3.6425608e-01  8.0951655e-01\n",
      " -3.4699607e-01 -3.0969405e-01  3.9612908e-02 -4.3319610e-01\n",
      " -3.2460812e-02  1.4265198e-01  3.5686022e-01 -3.3237728e-01\n",
      " -6.5476343e-02 -5.6264386e-02 -4.1712560e-02 -3.4480768e-01\n",
      "  1.6168292e-01 -3.5561968e-02  2.6941568e-01  1.6365443e-01\n",
      " -3.1138973e-02 -1.7730813e-01 -2.9514918e-01 -3.6047302e-02\n",
      " -3.2940161e-01 -1.8487844e-01  1.5671846e-01 -2.5505146e-02\n",
      "  2.3683523e-01 -2.0809671e-01  7.4777557e-03  4.6688542e-01\n",
      " -1.7832953e-02 -1.3996325e-01 -2.9677325e-01 -5.0111419e-01\n",
      "  6.6205958e-04 -1.2484885e-01  6.4427830e-02  1.3924873e-01\n",
      "  3.9282244e-01 -1.2897237e-01 -2.2194076e-01 -9.5699653e-02\n",
      "  2.6398894e-01  8.9673154e-02  8.5124977e-02 -4.7678670e-01\n",
      " -1.6975669e-02 -1.5707450e-01  1.4365081e-01 -2.2609833e-01\n",
      " -7.9880534e-03 -1.7302977e-01 -3.2180369e-01 -9.4598010e-02\n",
      "  1.2762365e-01  5.2694038e-02  1.4923738e-01 -8.5380048e-02\n",
      " -5.5988044e-01  2.6339090e-01  1.6539308e-01  2.9037815e-01\n",
      " -5.4558820e-01  5.0261629e-01 -5.4535475e-02  3.3435571e-01\n",
      "  4.8422569e-01  2.8100762e-01  5.3508204e-01  3.6347777e-01\n",
      "  1.5620105e-01 -4.9050402e-02 -1.9698285e-01 -1.0923246e-01\n",
      " -5.0979462e-03 -1.9295055e-01 -4.2294380e-01  4.7777891e-01\n",
      " -2.5204557e-01 -1.0558785e-01  4.9662955e-02 -6.0920534e-03\n",
      "  2.9494375e-01  1.5613745e-01  5.7913762e-01 -7.5572738e-03\n",
      "  1.2367397e-01  2.3106730e-01  8.1743991e-01  2.1333535e-01\n",
      "  2.2853145e-02 -9.5627621e-02  2.4072278e-01 -5.6870109e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding for word 'computer':\")\n",
    "print(word2vec_model.wv['computer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dd5431c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'computer':\n",
      "[('department', 0.9966564178466797), ('science', 0.994379460811615), ('state', 0.9940309524536133), ('chicago', 0.9919490218162537), ('hi', 0.99053555727005), ('corporation', 0.9904353022575378), ('laboratory', 0.9904119372367859), ('institute', 0.9900234937667847), ('engineering', 0.9899882078170776), ('world', 0.9891650676727295)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Most similar words to 'computer':\")\n",
    "print(word2vec_model.wv.most_similar('computer'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
